{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Biotechnology\n",
    "<span style=\"color:#AAA;font-size:14px;\" >Prof. Dr. Dominik Grimm</span>  \n",
    "<span style=\"color:#AAA;font-size:14px;\">Bioinformatics Research Lab</span>  \n",
    "<span style=\"color:#AAA;font-size:14px;\">TUM Campus Straubing for Biotechnology and Sustainability</span>  \n",
    "\n",
    "## Data Preprocessing and Normalization #7\n",
    "This exercise is based on some toy data. However, this toydata set mimics a real-world dataset, including missing values, samples in different order between the class labels and the feature matrix. In addition the features are not normalized or standardized. Because the dataset also contains strings we have to write our data import function using pure Python code (there exists other packages that might help with this, such as Pandas, but these packages also increase the complexity). For this purpose, we first implement a simple data parsen using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "samples_x = []\n",
    "\n",
    "f = open(\"toydata.txt\",\"r\")\n",
    "\n",
    "for line in f:\n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_x.append(sv[0])\n",
    "    \n",
    "    row = []\n",
    "    for element in sv[1:]:\n",
    "        row.append(float(element))\n",
    "    X.append(row)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "X = np.array(X)\n",
    "samples_x = np.array(samples_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `X` contains all features for all samples. Sample ids for each row are stored in the array `samples_x`.\n",
    "\n",
    "### Exercise 7.1\n",
    "Familiarise yourself with the data. How many samples and features does the data have? How many missing values are in X? What is the percentage of missing data? (Hint: have a look at the `numpy` function `np.isnan()`. This function helps to find `nan` values in a given array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m misssing_values\u001b[39m=\u001b[39m(np\u001b[39m.\u001b[39misnan(X))\u001b[39m.\u001b[39msum\n\u001b[1;32m----> 4\u001b[0m percentage_missing\u001b[39m=\u001b[39mmisssing_values\u001b[39m/\u001b[39mX\u001b[39m.\u001b[39;49msize()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "X.shape[0]\n",
    "X.shape[1]\n",
    "misssing_values=(np.isnan(X)).sum\n",
    "percentage_missing=misssing_values/X.size()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of missing values for each feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of missing values for each sample and store the indicies for each sample that has more than 70% missing values in a list with the name `sample_indices_to_remove`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 80.0\n",
      "135 80.0\n",
      "210 80.0\n",
      "338 80.0\n",
      "348 80.0\n",
      "542 90.0\n",
      "560 80.0\n",
      "574 90.0\n",
      "713 80.0\n",
      "862 80.0\n",
      "896 90.0\n",
      "932 80.0\n"
     ]
    }
   ],
   "source": [
    "sample_indeices_ro_remove=[]\n",
    "for i in range(X.shape[0]):\n",
    "    missing=np.isnan(X[i,:]).sum()/X.shape[1]*100\n",
    "    if missing >70:\n",
    "        print(i, missing)\n",
    "        sample_indeices_ro_remove.append(i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the samples from the index list `sample_indices_to_remove` from the matrix `X` and the array `samples_x` and print the number of samples to check if everything worked as excpeted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2\n",
    "Next we have to load the class labels from the file `toy_class_labels.txt`. The first column of the file stores the sample id and the second column the class label of the sample. We have to write a individual parser to load the information of that file. Store the sample ids in a list with the name `samples_y` and the class labels in a list with the name `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we match the data, we load all the Y and the samples for Y, we read it from the toy_class_labels\n",
    "#using the open function.\n",
    "#Then we split the columns and append it to the columns of the data set and finally transform it into a\n",
    "#numpy array\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to match the sample ids from the feature matrix with the samples ids from the class labels. Write some Python code the match both arrays. Finally, print the number of samples for `X` and `y` and write a routine to check if both `samples_x` and `samples_y` are the same (hint: check the numpy function `np.any()` ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can match the X and Y using the truth table to get a matrix that contains true and false values\n",
    "# We filter for the True values and then samples_x can be masked with the ind variable and y can \n",
    "#also be filtered and sorted\n",
    "#we can do a sanity check to understand if the order is matching using the np-any(samplesx==samplesy)\n",
    "#Sorted matched and filtered \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3\n",
    "Split your data into a training and testing set (20% of the samples should go into testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#First we split our data, we don't use the complete data using the function train_test_split \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4\n",
    "Next we have to impute the missing values on the training data. The feature at index 1 is a binary feature, the feature at index 3 is categorical and the remaining features are continues. First we will use mean imputation to mean impute the features ate indices `[0,2,4,5,6,7,8,9]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "continous_features = [0,2,4,5,6,7,8,9]\n",
    "other_features = [1,3]\n",
    "\n",
    "imputer_continous = SimpleImputer(strategy=\"mean\")\n",
    "#use indexing to only impute continous features\n",
    "#Then we would like to impute the data (Knn or the most frequent) we want to use a different \n",
    "#imputers for the different features categorical and continuous\n",
    "\n",
    "\n",
    "X_train[:,continous_features] = imputer_continous.fit_transform(X_train[:,continous_features]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the missing values for the binary and categorical features using the most frequent value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "imputer_category = 0 # replace with your code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your data using a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "ax = pl.subplot(111)\n",
    "#By observing the boxplot we observe that the data should be standardized and this never hurts us\n",
    "\n",
    "\n",
    "ax.grid(color=\"#CCCCCC\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "pl.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5\n",
    "After imputing the training data we will standardize the data having zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Write your code here\n",
    "scaler = 0 #replace with your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your standardized data again using a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "ax = pl.subplot(111)\n",
    "\n",
    "#The mean of each data set is now 0 and the model can be trained\n",
    "\n",
    "ax.grid(color=\"#CCCCCC\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "pl.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6\n",
    "Now we have to apply the imputation and standardization to our test data. Here we have to use the already initialized imputation and standardization objects from the training data. Use the `imputer_continous`, `imputer_category` and `scaler` objects from your previous tasks (they might have different names in your case) and apply them on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "#Impute missing values using the parameters from the training data\n",
    "#We use the imputer from before and we don't want to fit it but to transform the data we don't wanna \n",
    "#leak data\n",
    "#The same we use for the other features(before we had fit transform a two step function) \n",
    "\n",
    "#Standardize testing data using the mean and standard deviation from the training data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.7\n",
    "Finally we will train a simple logistic regression model without regularization on the training data and test the performance on the test data. Report common values, such as accuracy, precision, recall and MCC. Plot a ROC curve for your model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log: \t 3.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "#Now we keep it simple we don't do penalty or regularization but for illustration purposes its simple\n",
    "#We train the model using log reg using .fit\n",
    "\n",
    "#We find how this model performs by checking the scores of the test and then we predict using\n",
    "# model.predict and calculate the recall and mathew corr coeff for the test vs the pred\n",
    "a=3.0000\n",
    "\n",
    "print(\"Log: \\t %2f\"%a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
