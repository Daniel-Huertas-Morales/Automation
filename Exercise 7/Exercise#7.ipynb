{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Biotechnology\n",
    "<span style=\"color:#AAA;font-size:14px;\" >Prof. Dr. Dominik Grimm</span>  \n",
    "<span style=\"color:#AAA;font-size:14px;\">Bioinformatics Research Lab</span>  \n",
    "<span style=\"color:#AAA;font-size:14px;\">TUM Campus Straubing for Biotechnology and Sustainability</span>  \n",
    "\n",
    "## Data Preprocessing and Normalization #7\n",
    "This exercise is based on some toy data. However, this toydata set mimics a real-world dataset, including missing values, samples in different order between the class labels and the feature matrix. In addition the features are not normalized or standardized. Because the dataset also contains strings we have to write our data import function using pure Python code (there exists other packages that might help with this, such as Pandas, but these packages also increase the complexity). For this purpose, we first implement a simple data parsen using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "samples_x = []\n",
    "\n",
    "f = open(\"toydata.txt\",\"r\")\n",
    "\n",
    "for line in f:\n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_x.append(sv[0])\n",
    "    \n",
    "    row = []\n",
    "    for element in sv[1:]:\n",
    "        row.append(float(element))\n",
    "    X.append(row)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "X = np.array(X)\n",
    "samples_x = np.array(samples_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `X` contains all features for all samples. Sample ids for each row are stored in the array `samples_x`.\n",
    "\n",
    "### Exercise 7.1\n",
    "Familiarise yourself with the data. How many samples and features does the data have? How many missing values are in X? What is the percentage of missing data? (Hint: have a look at the `numpy` function `np.isnan()`. This function helps to find `nan` values in a given array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of missing values for each feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of missing values for each sample and store the indicies for each sample that has more than 70% missing values in a list with the name `sample_indices_to_remove`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the samples from the index list `sample_indices_to_remove` from the matrix `X` and the array `samples_x` and print the number of samples to check if everything worked as excpeted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2\n",
    "Next we have to load the class labels from the file `toy_class_labels.txt`. The first column of the file stores the sample id and the second column the class label of the sample. We have to write a individual parser to load the information of that file. Store the sample ids in a list with the name `samples_y` and the class labels in a list with the name `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to match the sample ids from the feature matrix with the samples ids from the class labels. Write some Python code the match both arrays. Finally, print the number of samples for `X` and `y` and write a routine to check if both `samples_x` and `samples_y` are the same (hint: check the numpy function `np.any()` ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3\n",
    "Split your data into a training and testing set (20% of the samples should go into testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4\n",
    "Next we have to impute the missing values on the training data. The feature at index 1 is a binary feature, the feature at index 3 is categorical and the remaining features are continues. First we will use mean imputation to mean impute the features ate indices `[0,2,4,5,6,7,8,9]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "continous_features = [0,2,4,5,6,7,8,9]\n",
    "other_features = [1,3]\n",
    "\n",
    "imputer_continous = SimpleImputer(strategy=\"mean\")\n",
    "#use indexing to only impute continous features\n",
    "X_train[:,continous_features] = imputer_continous.fit_transform(X_train[:,continous_features]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the missing values for the binary and categorical features using the most frequent value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "imputer_category = 0 # replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your data using a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "ax = pl.subplot(111)\n",
    "\n",
    "#Write your code here\n",
    "\n",
    "\n",
    "ax.grid(color=\"#CCCCCC\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5\n",
    "After imputing the training data we will standardize the data having zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Write your code here\n",
    "scaler = 0 #replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your standardized data again using a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "ax = pl.subplot(111)\n",
    "\n",
    "#Write your code here\n",
    "\n",
    "ax.grid(color=\"#CCCCCC\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6\n",
    "Now we have to apply the imputation and standardization to our test data. Here we have to use the already initialized imputation and standardization objects from the training data. Use the `imputer_continous`, `imputer_category` and `scaler` objects from your previous tasks (they might have different names in your case) and apply them on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "#Impute missing values using the parameters from the training data\n",
    "\n",
    "\n",
    "#Standardize testing data using the mean and standard deviation from the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.7\n",
    "Finally we will train a simple logistic regression model without regularization on the training data and test the performance on the test data. Report common values, such as accuracy, precision, recall and MCC. Plot a ROC curve for your model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
